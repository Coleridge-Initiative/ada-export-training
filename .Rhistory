sdc
sdc(data)
ods <- SD2011[1:1000,c("sex","age","edu","marital","income")]
ods
View(ods)
SD2011
s1 <- syn(ods, m = 2)
View(s1)
s1
s1 <- syn(data, m = 2)
s1
s1.sdc <- sdc(s1, ods, label="false_data", rm.replicated.uniques = TRUE,
s1.sdc <- sdc(s1, ods, label="false_data", rm.replicated.uniques = TRUE)
s1.sdc
s1.sdc <- sdc(s1, ods, label="false_data", rm.replicated.uniques = TRUE)
ods <- SD2011[1:1000,c("sex","age","edu","marital","income")]
s1 <- syn(ods, m = 2)
s1.sdc <- sdc(s1, ods, label="false_data", rm.replicated.uniques = TRUE)
s1.sdc
s1 <- syn(data, m = 2)
s1.sdc <- sdc(s1, data, label="false_data", rm.replicated.uniques = TRUE)
s1.sdc
localSupp()
library(sdcMicro)
install.packages("sdcMicro")
library(sdcMicro)
localSupp(data)
localSupp(data, .2)
freqCalc(data)
View(data)
freqCalc(data, keyVars = c(2,3,4))
new <- freqCalc(data, keyVars = c(2,3,4))
new
View(new)
View(data)
localSupp(new, .2)
localSupp(data, .2)
new <- freqCalc(data, keyVars = c(2,3,4))
localSupp(data, .2, keyVar = c(3,4))
localSupp(new, .2, keyVar = c(3,4))
localSupp(new, .2, keyVar = 4)
supp <- localSupp(new, .2, keyVar = 4)
supp
View(supp)
print(supp)
print(supp, type='ls')
indivRisk(new)
indivRisk(new)
a <- indivRisk(new)
a$rk
View(a)
install.packages("sdcMicro")
install.packages("sdcMicro")
sdcApp()
library(geojsonio)
library(leaflet)
library(dplyr)
library(ggplot2)
library(rjson)
library(shinythemes)
library(jsonlite)
library(leaflet)
library(RCurl)
library(jsonify)
library(htmlwidgets)
library( crosstalk )
library(shiny)
library(stringr)
library(rsconnect)
library(DT)
library(protolite)
data_df <- read.csv("part_list.csv")
data_df$Start.Year <- as.Date(data_df$Start.Date, format = '%d-%b-%y')
data_df$Start.Year <- format(data_df$Start.Year, format = "%Y")
data_df <- data_df %>% rename(lon = Longitude_Employer) %>% rename(lat = Latitude_Employer)
#get count of participants by organizations and start year. then get distinct values
map_data_df <- data_df %>% group_by(Org.clean1, Start.Year, Program) %>% mutate(n = n()) %>%
distinct(Org.clean1,n, Start.Year, lon, lat, Program) %>% ungroup()
# turn columns to numeric
map_data_df <- map_data_df %>%
mutate_at(vars(lat, lon), funs(as.numeric))
# get cumulative sum of participants by year. this will be fed into the output counter
cumulative_sum <- data_df %>% group_by(Start.Year) %>%
summarize(total = sum(n())) %>% mutate(csum = cumsum(total))
View(map_data_df)
map_data_df
map_data_df$Program
col1 <- c('All')
col1
p <- map_data_df$Program
a <- c('All')
f <- cbind(p,a)
f
p <- map_data_df$Program
a <- ('All')
a
p <- map_data_df$Program
f <- cbind(p,a)
f
map_data_df$Program <- cbind('all')
map_data_df$Program
map_data_df$Program <- cbind('All')
map_data_df$Program
data_df <- read.csv("part_list.csv")
data_df$Start.Year <- as.Date(data_df$Start.Date, format = '%d-%b-%y')
data_df$Start.Year <- format(data_df$Start.Year, format = "%Y")
data_df <- data_df %>% rename(lon = Longitude_Employer) %>% rename(lat = Latitude_Employer)
#get count of participants by organizations and start year. then get distinct values
map_data_df <- data_df %>% group_by(Org.clean1, Start.Year, Program) %>% mutate(n = n()) %>%
distinct(Org.clean1,n, Start.Year, lon, lat, Program) %>% ungroup()
# turn columns to numeric
map_data_df <- map_data_df %>%
mutate_at(vars(lat, lon), funs(as.numeric))
# get cumulative sum of participants by year. this will be fed into the output counter
cumulative_sum <- data_df %>% group_by(Start.Year) %>%
summarize(total = sum(n())) %>% mutate(csum = cumsum(total))
View(map_data_df)
data_df$Program
data_df$Program %>% cbind('All')
runApp('Dropbox/TrainingOverviewColeridgeInitiative/Code/Josh_mapping_viz_shiny')
runApp('Dropbox/TrainingOverviewColeridgeInitiative/Code/Josh_mapping_viz_shiny')
runApp('Dropbox/TrainingOverviewColeridgeInitiative/Code/Josh_mapping_viz_shiny')
runApp('Dropbox/TrainingOverviewColeridgeInitiative/Code/Josh_mapping_viz_shiny')
library(Rcrawler)
install.packages("Rcrawler")
library(Rcrawler)
RobotParser("http://www.glofile.com","AgentX")
RobotParser("http://www.glofile.com/robots.txt","AgentX")
RobotParser("http://www.glofile.com/robots.txt")
RobotParser("https://coleridgeinitiative.org/robots.txt")
RobotParser("https://coleridgeinitiative.org")
RobotParser("https://irs.gov")
RobotParser("https://coleridgeinitiative.org/training-programs/")
RobotParser("https://coleridgeinitiative.org/training-programs/", "*")
RobotParser("https://coleridgeinitiative.org", "*")
RobotParser("https://coleridgeinitiative.org/", "*")
RobotParser("https://coleridgeinitiative.org/wp-login.php?redirect_to=https%3A%2F%2Fcoleridgeinitiative.org%2Fwp-admin%2F&reauth=1", "*")
RobotParser(("coleridgeinitiative.org"))
RobotParser(("coleridgeinitiative.org/wp-admin"))
RobotParser(("coleridgeinitiative.org/wp-admin/"))
RobotParser(("coleridgeinitiative.org/wp-admin/", "*"))
RobotParser(("coleridgeinitiative.org/wp-admin", "*"))
RobotParser("coleridgeinitiative.org/wp-admin", "*")
RobotParser("https://coleridgeinitiative.org/wp-admin/", "*")
RobotParser("https://coleridgeinitiative.org/wp-admin", "*")
RobotParser("https://coleridgeinitiative.org", "*")
RobotParser("https://coleridgeinitiative.org", "*")[3]
RobotParser("irs.gov", "*")[3]
RobotParser("cubs.com", "*")[3]
RobotParser("espn.com", "*")[3]
RobotParser("colorado.edu", "*")[3]
RobotParser("r.com", "*")[3]
RobotParser("realtor.com", "*")[3]
RobotParser("redfin.com", "*")[3]
RobotParser("stockx.com", "*")[3]
RobotParser("https://stockx.com/", "*")[3]
RobotParser("https://accounts.stockx.com/login?state=hKFo2SBIZTZ3UkZyNGd5eVh2V2lSYWpfNWhLdEtnZUZud2E4SqFupWxvZ2luo3RpZNkgU051Mk5jLTRobDB6VzNSaDRjUHNLaThNNEM2U1d0RGWjY2lk2SBPVnhydDRWSnFUeDdMSVVLZDY2MVcwRHVWTXBjRkJ5RA&client=OVxrt4VJqTx7LIUKd661W0DuVMpcFByD&protocol=oauth2&audience=gateway.stockx.com&auth0Client=eyJuYW1lIjoiYXV0aDAuanMiLCJ2ZXJzaW9uIjoiOS4xOS4wIn0%3D&connection=production&lng=en&redirect_uri=https%3A%2F%2Fstockx.com%2Fcallback%3Fpath%3D%2Fsettings&response_mode=query&response_type=code&scope=openid%20profile&stockx-currency=USD&stockx-default-tab=signup&stockx-is-gdpr=false&stockx-language=en-us&stockx-session-id=202d5370-d1d6-452f-b75b-4909a2cf46ee&stockx-url=https%3A%2F%2Fstockx.com&stockx-user-agent=Mozilla%2F5.0%20(Macintosh%3B%20Intel%20Mac%20OS%20X%2010_15_7)%20AppleWebKit%2F537.36%20(KHTML%2C%20like%20Gecko)%20Chrome%2F99.0.4844.74%20Safari%2F537.36&ui_locales=en", "*")[3]
RobotParser("stockx.com/settings", "*")
RobotParser <- function(website, useragent) {
URLrobot<-paste(website,"/robots.txt", sep = "")
bots<-httr::GET(URLrobot, httr::user_agent("Mozilla/5.0 (Windows NT 6.3; WOW64; rv:42.0) Gecko/20100101 Firefox/42.0"),httr::timeout(5))
bots<-as.character(httr::content(bots, as="text", encoding = "UTF-8"))
write(bots, file = "robots.txt")
bots <- readLines("robots.txt") # dans le repertoire du site
if (missing(useragent)) useragent<-"Rcrawler"
useragent <- c(useragent, "*")
ua_positions <- which(grepl( "[Uu]ser-[Aa]gent:[ ].+", bots))
Disallow_dir<-vector()
allow_dir<-vector()
for (i in 1:length(useragent)){
if (useragent[i] == "*") useragent[i]<-"\\*"
Gua_pos <- which(grepl(paste("[Uu]ser-[Aa]gent:[ ]{0,}", useragent[i], "$", sep=""),bots))
if (length(Gua_pos)!=0 ){
Gua_rules_start <- Gua_pos+1
Gua_rules_end <- ua_positions[which(ua_positions==Gua_pos)+1]-1
if(is.na(Gua_rules_end)) Gua_rules_end<- length(bots)
Gua_rules <- bots[Gua_rules_start:Gua_rules_end]
Disallow_rules<-Gua_rules[grep("[Dd]isallow",Gua_rules)]
Disallow_dir<-c(Disallow_dir,gsub(".*\\:.","",Disallow_rules))
allow_rules<-Gua_rules[grep("^[Aa]llow",Gua_rules)]
allow_dir<-c(allow_dir,gsub(".*\\:.","",allow_rules))
}
}
if ("/" %in% Disallow_dir){
Blocked=TRUE
print ("This bot is blocked from the site")} else{ Blocked=FALSE }
Rules<-list(Allow=allow_dir,Disallow=Disallow_dir,Blocked=Blocked )
return (Rules)
}
print(Disallow_dir)
return (Rules)
RobotParser <- function(website, useragent) {
URLrobot<-paste(website,"/robots.txt", sep = "")
bots<-httr::GET(URLrobot, httr::user_agent("Mozilla/5.0 (Windows NT 6.3; WOW64; rv:42.0) Gecko/20100101 Firefox/42.0"),httr::timeout(5))
bots<-as.character(httr::content(bots, as="text", encoding = "UTF-8"))
write(bots, file = "robots.txt")
bots <- readLines("robots.txt") # dans le repertoire du site
if (missing(useragent)) useragent<-"Rcrawler"
useragent <- c(useragent, "*")
ua_positions <- which(grepl( "[Uu]ser-[Aa]gent:[ ].+", bots))
Disallow_dir<-vector()
allow_dir<-vector()
for (i in 1:length(useragent)){
if (useragent[i] == "*") useragent[i]<-"\\*"
Gua_pos <- which(grepl(paste("[Uu]ser-[Aa]gent:[ ]{0,}", useragent[i], "$", sep=""),bots))
if (length(Gua_pos)!=0 ){
Gua_rules_start <- Gua_pos+1
Gua_rules_end <- ua_positions[which(ua_positions==Gua_pos)+1]-1
if(is.na(Gua_rules_end)) Gua_rules_end<- length(bots)
Gua_rules <- bots[Gua_rules_start:Gua_rules_end]
Disallow_rules<-Gua_rules[grep("[Dd]isallow",Gua_rules)]
Disallow_dir<-c(Disallow_dir,gsub(".*\\:.","",Disallow_rules))
print(Disallow_dir)
allow_rules<-Gua_rules[grep("^[Aa]llow",Gua_rules)]
allow_dir<-c(allow_dir,gsub(".*\\:.","",allow_rules))
}
}
if ("/" %in% Disallow_dir){
Blocked=TRUE
print ("This bot is blocked from the site")} else{ Blocked=FALSE }
Rules<-list(Allow=allow_dir,Disallow=Disallow_dir,Blocked=Blocked )
return (Rules)
}
RobotParser("stockx.com/settings", "*")
RobotParser <- function(website, useragent) {
URLrobot<-paste(website,"/robots.txt", sep = "")
bots<-httr::GET(URLrobot, httr::user_agent("Mozilla/5.0 (Windows NT 6.3; WOW64; rv:42.0) Gecko/20100101 Firefox/42.0"),httr::timeout(5))
bots<-as.character(httr::content(bots, as="text", encoding = "UTF-8"))
write(bots, file = "robots.txt")
bots <- readLines("robots.txt") # dans le repertoire du site
if (missing(useragent)) useragent<-"Rcrawler"
useragent <- c(useragent, "*")
ua_positions <- which(grepl( "[Uu]ser-[Aa]gent:[ ].+", bots))
Disallow_dir<-vector()
allow_dir<-vector()
for (i in 1:length(useragent)){
if (useragent[i] == "*") useragent[i]<-"\\*"
Gua_pos <- which(grepl(paste("[Uu]ser-[Aa]gent:[ ]{0,}", useragent[i], "$", sep=""),bots))
if (length(Gua_pos)!=0 ){
Gua_rules_start <- Gua_pos+1
Gua_rules_end <- ua_positions[which(ua_positions==Gua_pos)+1]-1
if(is.na(Gua_rules_end)) Gua_rules_end<- length(bots)
Gua_rules <- bots[Gua_rules_start:Gua_rules_end]
Disallow_rules<-Gua_rules[grep("[Dd]isallow",Gua_rules)]
Disallow_dir<-c(Disallow_dir,gsub(".*\\:.","",Disallow_rules))
print (Disallow_rules)
allow_rules<-Gua_rules[grep("^[Aa]llow",Gua_rules)]
allow_dir<-c(allow_dir,gsub(".*\\:.","",allow_rules))
}
}
if ("/" %in% Disallow_dir){
Blocked=TRUE
print ("This bot is blocked from the site")} else{ Blocked=FALSE }
Rules<-list(Allow=allow_dir,Disallow=Disallow_dir,Blocked=Blocked )
return (Rules)
}
RobotParser("stockx.com/settings", "*")
RobotParser <- function(website, useragent) {
URLrobot<-paste(website,"/robots.txt", sep = "")
bots<-httr::GET(URLrobot, httr::user_agent("Mozilla/5.0 (Windows NT 6.3; WOW64; rv:42.0) Gecko/20100101 Firefox/42.0"),httr::timeout(5))
bots<-as.character(httr::content(bots, as="text", encoding = "UTF-8"))
write(bots, file = "robots.txt")
bots <- readLines("robots.txt") # dans le repertoire du site
if (missing(useragent)) useragent<-"Rcrawler"
useragent <- c(useragent, "*")
ua_positions <- which(grepl( "[Uu]ser-[Aa]gent:[ ].+", bots))
Disallow_dir<-vector()
allow_dir<-vector()
for (i in 1:length(useragent)){
if (useragent[i] == "*") useragent[i]<-"\\*"
Gua_pos <- which(grepl(paste("[Uu]ser-[Aa]gent:[ ]{0,}", useragent[i], "$", sep=""),bots))
if (length(Gua_pos)!=0 ){
Gua_rules_start <- Gua_pos+1
Gua_rules_end <- ua_positions[which(ua_positions==Gua_pos)+1]-1
if(is.na(Gua_rules_end)) Gua_rules_end<- length(bots)
Gua_rules <- bots[Gua_rules_start:Gua_rules_end]
Disallow_rules<-Gua_rules[grep("[Dd]isallow",Gua_rules)]
Disallow_dir<-c(Disallow_dir,gsub(".*\\:.","",Disallow_rules))
allow_rules<-Gua_rules[grep("^[Aa]llow",Gua_rules)]
allow_dir<-c(allow_dir,gsub(".*\\:.","",allow_rules))
print(allow_dir)
}
}
if ("/" %in% Disallow_dir){
Blocked=TRUE
print ("This bot is blocked from the site")} else{ Blocked=FALSE }
Rules<-list(Allow=allow_dir,Disallow=Disallow_dir,Blocked=Blocked )
return (Rules)
}
RobotParser("stockx.com/settings", "*")
RobotParser <- function(website, useragent) {
URLrobot<-paste(website,"/robots.txt", sep = "")
bots<-httr::GET(URLrobot, httr::user_agent("Mozilla/5.0 (Windows NT 6.3; WOW64; rv:42.0) Gecko/20100101 Firefox/42.0"),httr::timeout(5))
bots<-as.character(httr::content(bots, as="text", encoding = "UTF-8"))
write(bots, file = "robots.txt")
bots <- readLines("robots.txt") # dans le repertoire du site
if (missing(useragent)) useragent<-"Rcrawler"
useragent <- c(useragent, "*")
ua_positions <- which(grepl( "[Uu]ser-[Aa]gent:[ ].+", bots))
Disallow_dir<-vector()
allow_dir<-vector()
for (i in 1:length(useragent)){
if (useragent[i] == "*") useragent[i]<-"\\*"
Gua_pos <- which(grepl(paste("[Uu]ser-[Aa]gent:[ ]{0,}", useragent[i], "$", sep=""),bots))
if (length(Gua_pos)!=0 ){
Gua_rules_start <- Gua_pos+1
Gua_rules_end <- ua_positions[which(ua_positions==Gua_pos)+1]-1
if(is.na(Gua_rules_end)) Gua_rules_end<- length(bots)
Gua_rules <- bots[Gua_rules_start:Gua_rules_end]
Disallow_rules<-Gua_rules[grep("[Dd]isallow",Gua_rules)]
Disallow_dir<-c(Disallow_dir,gsub(".*\\:.","",Disallow_rules))
allow_rules<-Gua_rules[grep("^[Aa]llow",Gua_rules)]
allow_dir<-c(allow_dir,gsub(".*\\:.","",allow_rules))
print(allow_dir)
}
}
if ("/" %in% Disallow_dir){
Blocked=FALSE
print ("This bot is blocked from the site")} else{ Blocked=FALSE }
Rules<-list(Allow=allow_dir,Disallow=Disallow_dir,Blocked=Blocked )
return (Rules)
}
RobotParser("stockx.com/settings", "*")
RobotParser("stockx.com/settings", "*")
RobotParser <- function(website, useragent) {
URLrobot<-paste(website,"/robots.txt", sep = "")
bots<-httr::GET(URLrobot, httr::user_agent("Mozilla/5.0 (Windows NT 6.3; WOW64; rv:42.0) Gecko/20100101 Firefox/42.0"),httr::timeout(5))
bots<-as.character(httr::content(bots, as="text", encoding = "UTF-8"))
write(bots, file = "robots.txt")
bots <- readLines("robots.txt") # dans le repertoire du site
if (missing(useragent)) useragent<-"Rcrawler"
useragent <- c(useragent, "*")
ua_positions <- which(grepl( "[Uu]ser-[Aa]gent:[ ].+", bots))
Disallow_dir<-vector()
allow_dir<-vector()
for (i in 1:length(useragent)){
if (useragent[i] == "*") useragent[i]<-"\\*"
Gua_pos <- which(grepl(paste("[Uu]ser-[Aa]gent:[ ]{0,}", useragent[i], "$", sep=""),bots))
if (length(Gua_pos)!=0 ){
Gua_rules_start <- Gua_pos+1
Gua_rules_end <- ua_positions[which(ua_positions==Gua_pos)+1]-1
if(is.na(Gua_rules_end)) Gua_rules_end<- length(bots)
Gua_rules <- bots[Gua_rules_start:Gua_rules_end]
Disallow_rules<-Gua_rules[grep("[Dd]isallow",Gua_rules)]
Disallow_dir<-c(Disallow_dir,gsub(".*\\:.","",Disallow_rules))
allow_rules<-Gua_rules[grep("^[Aa]llow",Gua_rules)]
allow_dir<-c(allow_dir,gsub(".*\\:.","",allow_rules))
print(allow_dir)
}
}
if ("/" %in% Disallow_dir){
Blocked=FALSE
print ("This bot is blocked from the site")} else{ Blocked=TRUE }
Rules<-list(Allow=allow_dir,Disallow=Disallow_dir,Blocked=Blocked )
return (Rules)
}
RobotParser("stockx.com/settings", "*")
RobotParser <- function(website, useragent) {
URLrobot<-paste(website,"/robots.txt", sep = "")
bots<-httr::GET(URLrobot, httr::user_agent("Mozilla/5.0 (Windows NT 6.3; WOW64; rv:42.0) Gecko/20100101 Firefox/42.0"),httr::timeout(5))
bots<-as.character(httr::content(bots, as="text", encoding = "UTF-8"))
write(bots, file = "robots.txt")
bots <- readLines("robots.txt") # dans le repertoire du site
if (missing(useragent)) useragent<-"Rcrawler"
useragent <- c(useragent, "*")
ua_positions <- which(grepl( "[Uu]ser-[Aa]gent:[ ].+", bots))
Disallow_dir<-vector()
allow_dir<-vector()
for (i in 1:length(useragent)){
if (useragent[i] == "*") useragent[i]<-"\\*"
Gua_pos <- which(grepl(paste("[Uu]ser-[Aa]gent:[ ]{0,}", useragent[i], "$", sep=""),bots))
if (length(Gua_pos)!=0 ){
Gua_rules_start <- Gua_pos+1
Gua_rules_end <- ua_positions[which(ua_positions==Gua_pos)+1]-1
if(is.na(Gua_rules_end)) Gua_rules_end<- length(bots)
Gua_rules <- bots[Gua_rules_start:Gua_rules_end]
Disallow_rules<-Gua_rules[grep("[Dd]isallow",Gua_rules)]
Disallow_dir<-c(Disallow_dir,gsub(".*\\:.","",Disallow_rules))
allow_rules<-Gua_rules[grep("^[Aa]llow",Gua_rules)]
allow_dir<-c(allow_dir,gsub(".*\\:.","",allow_rules))
print(allow_dir)
}
}
if ("/" %in% Disallow_dir){
Blocked=TRUE
print ("This bot is blocked from the site")} else{ Blocked=TRUE }
Rules<-list(Allow=allow_dir,Disallow=Disallow_dir,Blocked=Blocked )
return (Rules)
}
RobotParser("stockx.com/settings", "*")
library(Rcrawler)
RobotParser("stockx.com/settings", "*")
library(Rcrawler)
RobotParser("stockx.com/settings", "*")
RobotParser("stockx.com/", "*")
RobotParser("stockx.com", "*")
RobotParser("stockx.com/signup", "*")
RobotParser("stockx.com/", "*")
RobotParser("stockx.com", "*")
RobotParser("stockx.com/api/", "*")
library(shiny)
ui <- fluidPage(
textInput('input', 'Enter input'),
verbatimTextOutput('output')
)
server <- function(input, output) {
output$output <- renderText({
req(input$input)
strsplit(input$input, '')[[1]]
})
}
shinyApp(ui, server)
ui <- fluidPage(
textInput('input', 'Enter website'),
textOutput('output')
)
server <- function(input, output) {
output$output <- renderText({
req(input$input)
strsplit(input$input, '')[[1]]
})
}
shinyApp(ui, server)
ui <- fluidPage(
textInput('input', 'website'),
textOutput('output')
)
server <- function(input, output) {
output$output <- renderText({
req(input$website)
strsplit(input$website, '')[[1]]
})
}
shinyApp(ui, server)
ui <- fluidPage(
textInput('input', 'website'),
textOutput('output')
)
server <- function(input, output) {
output$output <- renderText({
req(input$input)
RobotParser("stockx.com/api/", "*")
})
}
shinyApp(ui, server)
library(Rcrawler)
ui <- fluidPage(
textInput('input', 'website'),
textOutput('output')
)
server <- function(input, output) {
output$output <- renderText({
req(input$input)
RobotParser("stockx.com/api/", "*")
})
}
shinyApp(ui, server)
library(shiny)
ui <- fluidPage(
textInput('input', 'website'),
textOutput('output')
)
server <- function(input, output) {
output$output <- renderText({
req(input$input)
RobotParser("stockx.com/api/", "*")
})
}
shinyApp(ui, server)
ui <- fluidPage(
textInput('input', 'website'),
textOutput('output')
)
server <- function(input, output) {
output$output <- renderText({
RobotParser("stockx.com/api/", "*")
})
}
shinyApp(ui, server)
shinyApp(ui, server)
runApp('Documents/coleridgeFiles/web scraping /web_scrape.R')
ui <- fluidPage(
textOutput("text"),
verbatimTextOutput("code")
)
server <- function(input, output, session) {
output$text <- renderText({
"Hello friend!"
})
output$code <- renderPrint({
summary(1:10)
})
}
runApp('Documents/coleridgeFiles/web scraping /web_scrape.R')
runApp('Documents/coleridgeFiles/web scraping /web_scrape.R')
runApp('Documents/coleridgeFiles/web scraping /web_scrape.R')
runApp('Documents/coleridgeFiles/web scraping /web_scrape.R')
runApp('Documents/coleridgeFiles/web scraping /web_scrape.R')
runApp('Documents/coleridgeFiles/web scraping /web_scrape.R')
runApp('Documents/coleridgeFiles/web scraping /web_scrape.R')
runApp('Documents/coleridgeFiles/web scraping /web_scrape.R')
runApp('Documents/coleridgeFiles/web scraping /web_scrape.R')
runApp('Documents/coleridgeFiles/web scraping /web_scrape.R')
runApp('Documents/coleridgeFiles/web scraping /web_scrape.R')
runApp('Documents/coleridgeFiles/web scraping /web_scrape.R')
runApp('Documents/coleridgeFiles/web scraping /web_scrape.R')
runApp('Documents/coleridgeFiles/web scraping /web_scrape.R')
library(freqpoly)
library(ggplot2)
runApp('Documents/coleridgeFiles/web scraping /web_scrape.R')
runApp('Documents/coleridgeFiles/web scraping /web_scrape.R')
setwd("~/Documents/coleridgeFiles/git_repos/ada-export-training")
